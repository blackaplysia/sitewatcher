#!/usr/bin/python3

import difflib
import filetype
import hashlib
import logging
import os
import re
import requests
import sys
import time
from redis import Redis
from bs4 import BeautifulSoup
from urllib.parse import urljoin

debug_mode = False

logger = logging.getLogger()
logger.setLevel(logging.DEBUG)

redis_host = os.environ.get('REDIS_HOST', 'localhost')
redis_port = os.environ.get('REDIS_PORT', '6379')
redis_ttl = os.environ.get('REDIS_TTL', '60')
redis = Redis(host=redis_host, port=redis_port, decode_responses=True)

redis_skey_index = 'index'
redis_skey_hashes = 'hashes'
redis_skey_latests = 'latests'

redis_hkey_name = 'name'
redis_hkey_url = 'url'
redis_hkey_depth = 'depth'
redis_hkey_link = 'link'
redis_hkey_parent = 'parent'
redis_hkey_tag = 'tag'
redis_hkey_site = 'site'

def add_redis_name(name, resid):
    name_lower = name.lower()
    redis.set(name_lower, resid)
    redis.hset(resid, redis_hkey_name, name)
    redis.sadd(redis_skey_index, name_lower)

def delete_redis_name(name, resid):
    name_lower = name.lower()
    redis.srem(redis_skey_index, name_lower)
    redis.delete(name_lower)
    redis.delete(resid)

def rename_redis_name(old_name, new_name, resid):
    old_name_lower = old_name.lower()
    new_name_lower = new_name.lower()
    redis.rename(old_name_lower, new_name_lower)
    redis.hset(resid, redis_hkey_name, new_name)
    redis.sadd(redis_skey_index, new_name_lower)
    redis.srem(redis_skey_index, old_name_lower)

def get_redis_names():
    return redis.smembers(redis_skey_index)

def get_redis_resid(name):
    name_lower = name.lower()
    return redis.get(name_lower)

def set_redis_value(resid, hkey, hvalue):
    redis.hset(resid, hkey, hvalue)

def get_redis_value(resid, hkey):
    return redis.hget(resid, hkey)

def delete_redis_value(resid, hkey):
    return redis.hdel(resid, hkey)

def add_redis_smember(resid, skey, svalue):
    redis.sadd(resid + '+' + skey, svalue)
    redis.sadd(skey, svalue)

def remove_redis_smember(resid, skey, svalue):
    redis.srem(resid + '+' + skey, svalue)

def flush_redis_smembers(resid, skey):
    redis.delete(resid + '+' + skey)

def get_redis_smembers(resid, skey):
    return redis.smembers(resid + '+' + skey)

class Site:

    def __init__(self, name):
        name = name
        self.name = name
        self.resid = get_redis_resid(self.name)
        if self.resid is None:
            self.resid = hashlib.md5(self.name.encode()).hexdigest()
            self.url = None
            self.depth = None
            self.hashes = []
            self.latests = []
            self.exists = False
        else:
            self.name = get_redis_value(self.resid, redis_hkey_name)
            self.url = get_redis_value(self.resid, redis_hkey_url)
            self.depth = int(get_redis_value(self.resid, redis_hkey_depth))
            self.hashes = list(get_redis_smembers(self.resid, redis_skey_hashes))
            self.latests = list(get_redis_smembers(self.resid, redis_skey_latests))
            self.exists = True

    def add(self, url, depth):
        if self.exists == True:
            logger.error('already exists.')
            return False

        if url is None:
            logger.error('invalid url')
            return False

        if depth < 1:
            logger.error('invalid depth')
            return False

        add_redis_name(self.name, self.resid)
        set_redis_value(self.resid, redis_hkey_url, url)
        set_redis_value(self.resid, redis_hkey_depth, depth)
        self.exists = True

        return True

    def delete(self):
        if self.exists == False:
            logger.error('{}: no such a site.'.format(self.name))
            return False

        delete_redis_name(self.name, self.resid)
        self.exists = False
        return True

    def rename(self, new_name):
        if self.exists == False:
            logger.error('{}: no such a site.'.format(self.name))
            return False

        old_name = self.name
        self.name = new_name
        rename_redis_name(old_name, self.name, self.resid)
        return True

    def get_title(self, name, url, parent_name):
        title = name
        res = None
        try:
            res = requests.get(url)
        except requests.exceptions.InvalidSchema as e:
            logger.debug(e)
        except requests.exceptions.RequestException as e:
            logger.warning('{}: failed to fetch {}'.format(self.name, url))
            logger.debug(e)
        except Exception as e:
            logger.warning('{}: failed to fetch {}'.format(self.name, url))
            logger.debug(e)

        if res is not None:
            if res.status_code >= 400:
                logger.warning('{}: failed to fetch {}. Status code={}'.format(self.name, url, res.status_code))
            else:
                ftype = filetype.guess(res.content)
                if ftype:
                    if parent_name is None:
                        parent_name = ""
                    title = '[' + ftype.extension + ']' + parent_name + '::' + title
                    logger.debug("name changed: {} -> {}".format(name, title))
                else:
                    real_title = None
                    enc = res.encoding if res.encoding != 'ISO-8859-1' else None
                    bs = BeautifulSoup(res.content, 'html.parser', from_encoding=enc)
                    bs_tag = bs.find('title')
                    bs_ogp = bs.find('meta', attrs={'property': 'og:title'})
                    if bs_ogp is not None:
                        real_title = bs_ogp.get('content')
                    elif bs_tag is not None:
                        real_title = bs_tag.get_text()
                    if real_title is not None and len(real_title) > 0:
                        title = title + '::' + real_title
                    logger.debug("name changed: {} -> {}".format(name, title))

        return title

    def get_references(self, bs, parent_url, links, parent=None):
        links = {}
        for t in bs.find_all('a'):
            ref = t.get('href')
            if ref is not None:
                ref = ''.join(filter(lambda c: c >= ' ', ref))
                if parent_url is not None:
                    ref = urljoin(parent_url, ref)
                if ref.startswith('javascript:'):
                    pass
                elif ref.startswith('http://warp.da.ndl.go.jp/collections/'):
                    pass
                else:
                    cs = t.strings
                    if cs is not None:
                        title = re.sub('[ã€€ ]+', ' ', '::'.join(filter(lambda x: len(x) > 0, [s.strip() for s in cs])))
                    tag = title + (' ' if len(title) > 0 else '') + '---- ' + ref
                    hash = hashlib.md5((self.resid + tag).encode()).hexdigest()
                    if hash not in links:
                        links.update({hash: { 'site': self.resid, 'parent': parent, 'name': title, 'link': ref, 'tag': tag }})
        return links

    def make_link_set(self, url, depth, links, parent=None):

        res = None
        try:
            res = requests.get(url)
        except requests.exceptions.RequestException as e:
            logger.warning('{}: failed to fetch {}'.format(self.name, url))
            logger.debug(e)
            return None
        except Exception as e:
            logger.warning('{}: failed to fetch {}'.format(self.name, url))
            logger.debug(e)
            return None

        if res.status_code >= 400:
            logger.warning('{}: failed to fetch {}. Status code={}'.format(self.name, url, res.status_code))
            return None

        bs = BeautifulSoup(res.content, 'html.parser')
        children = self.get_references(bs, url, links, parent)
        links.update(children)

        if depth > 1:
            for h in children:
                descendant_links = self.make_link_set(children[h]['link'], depth - 1, links, h)
            links.update(descendant_links)

        return links

    def update(self):
        if self.exists == False:
            logger.error('{}: no such a site.'.format(self.name))
            return False

        links = self.make_link_set(self.url, self.depth, {})
        if links is None:
            logger.warning('{}: no links found'.format(self.name))
        else:
            hashes = links.keys()

            latests = list(set(hashes) - set(self.hashes))
            obsoletes = list(set(self.hashes) - set(hashes))

            logger.debug('{}: old: {}'.format(self.name, self.hashes))
            logger.debug('{}: new: {}'.format(self.name, hashes))
            logger.debug('{}: latests: {}'.format(self.name, latests))
            logger.debug('{}: obsoletes: {}'.format(self.name, obsoletes))

            if len(latests) > 0:
                for h in latests:
                    parent_name = None
                    parent = links[h]['parent']
                    if parent is not None:
                        parent_name = links[parent]['name']
                    title = self.get_title(links[h]['name'], links[h]['link'], parent_name)
                    if len(title) > 0:
                        links[h]['name'] = title
                        links[h]['tag'] = title + ' ---- ' + links[h]['link']
                    add_redis_smember(self.resid, redis_skey_hashes, h)
                    set_redis_value(h, redis_hkey_site, links[h]['site'])
                    set_redis_value(h, redis_hkey_name, links[h]['name'])
                    set_redis_value(h, redis_hkey_link, links[h]['link'])
                    if links[h]['parent'] is not None:
                        set_redis_value(h, redis_hkey_parent, links[h]['parent'])
                    set_redis_value(h, redis_hkey_tag, links[h]['tag'])
                    logger.debug('{}: added: {} {}'.format(self.name, h, links[h]['tag']))

                if len(self.hashes) > 0:
                    logger.debug('{}: first update'.format(self.name))
                else:
                    flush_redis_smembers(self.resid, redis_skey_latests)
                    for h in latests:
                        add_redis_smember(self.resid, redis_skey_latests, h)

                logger.info('{}: updated'.format(self.name))

            for h in obsoletes:
                remove_redis_smember(self.resid, redis_skey_hashes, h)
                obsolete_tag = get_redis_value(h, redis_hkey_tag)
                delete_redis_value(h, redis_hkey_site)
                delete_redis_value(h, redis_hkey_name)
                delete_redis_value(h, redis_hkey_link)
                delete_redis_value(h, redis_hkey_parent)
                delete_redis_value(h, redis_hkey_tag)
                logger.debug('{}: removed: {} {}'.format(self.name, h, obsolete_tag))

        return True

    def links(self):
        if self.exists == False:
            logger.error('{}: no such a site.'.format(self.name))
            return False

        if self.hashes is not None:
            for h in self.hashes:
                tag = get_redis_value(h, redis_hkey_tag)
                if debug_mode is True:
                    print("{} {} {}".format(h, self.name, tag))
                else:
                    print("{} {}".format(self.name, tag))

    def print(self):
        if self.exists == False:
            logger.error('{}: no such a site.'.format(self.name))
            return False

        if self.latests is not None:
            for h in self.latests:
                tag = get_redis_value(h, redis_hkey_tag)
                if debug_mode is True:
                    print("{} {} {}".format(h, self.name, tag))
                else:
                    print("{} {}".format(self.name, tag))

class SiteList:

    def __init__(self):
        self.site_name_list = sorted(list(get_redis_names()))

    def list(self):
        for s in self.site_name_list:
            resid = get_redis_resid(s)
            name = get_redis_value(resid, redis_hkey_name)
            url = get_redis_value(resid, redis_hkey_url)
            depth = get_redis_value(resid, redis_hkey_depth)
            print('{} {} {} {}'.format(resid, name, url, depth))

    def update_all(self):
        for s in self.site_name_list:
            resid = get_redis_resid(s)
            name = get_redis_value(resid, redis_hkey_name)
            Site(name).update()

    def print_all(self):
        for s in self.site_name_list:
            resid = get_redis_resid(s)
            name = get_redis_value(resid, redis_hkey_name)
            Site(name).print()

    def links_all(self):
        for s in self.site_name_list:
            resid = get_redis_resid(s)
            name = get_redis_value(resid, redis_hkey_name)
            Site(name).links()

if __name__ == '__main__':

    import argparse
    import csv
    from argparse import HelpFormatter
    from operator import attrgetter
    class SortingHelpFormatter(HelpFormatter):
        def add_arguments(self, actions):
            actions = sorted(actions, key=attrgetter('option_strings'))
            super(SortingHelpFormatter, self).add_arguments(actions)

    parser = argparse.ArgumentParser(description='Check updating of web sites', formatter_class=SortingHelpFormatter)
    parser.add_argument('--debug', action='store_true', help='debug output')

    sps = parser.add_subparsers(dest='subparser_name', title='action arguments')
    sp_add = sps.add_parser('add', help='add a site')
    sp_add.add_argument('name', nargs=1, metavar='NAME', help='site name')
    sp_add.add_argument('url', nargs=1, metavar='URL', help='site url')
    sp_add.add_argument('depth', nargs='?', default='1', metavar='DEPTH', help='depth')
    sp_delete = sps.add_parser('delete', help='delete a site')
    sp_delete.add_argument('name', nargs=1, metavar='NAME', help='site name')
    sp_rename = sps.add_parser('rename', help='rename a site')
    sp_rename.add_argument('name', nargs=1, metavar='NAME', help='site name')
    sp_rename.add_argument('new_name', nargs=1, metavar='NEW_NAME', help='new name')
    sp_update = sps.add_parser('update', help='update a site or all sites')
    sp_update.add_argument('name', nargs=1, metavar='NAME', help='site name (\'all\' for all sites)')
    sp_links = sps.add_parser('links', help='print all links of a site or all sites')
    sp_links.add_argument('name', nargs=1, metavar='NAME', help='site name (\'all\' for all sites)')
    sp_print = sps.add_parser('print', help='print latest links of a site or all sites')
    sp_print.add_argument('name', nargs=1, metavar='NAME', help='site name (\'all\' for all sites)')
    sp_list = sps.add_parser('list', help='list sites')

    if len(sys.argv) == 1:
        print(parser.format_usage(), file=sys.stderr)
        exit(0)

    args = parser.parse_args()
    debug_mode = args.debug

    log_format = '%(levelname)s: %(message)s'
    log_level = logging.INFO
    if debug_mode is True:
        log_format = '%(asctime)s %(name)-12s %(levelname)-8s %(message)s'
        log_level = logging.DEBUG
    console = logging.StreamHandler()
    console.setFormatter(logging.Formatter(log_format))
    console.setLevel(log_level)
    logger.addHandler(console)

    if args.subparser_name == 'add':
        name = args.name[0].strip()
        url = args.url[0].strip()
        depth = int(args.depth)
        Site(name).add(url, depth)
    elif args.subparser_name == 'delete':
        name = args.name[0].strip()
        Site(name).delete()
    elif args.subparser_name == 'rename':
        name = args.name[0].strip()
        Site(name).rename(args.new_name[0])
    elif args.subparser_name == 'update':
        name = args.name[0].strip()
        if name.lower() == 'all':
            SiteList().update_all()
        else:
            Site(name).update()
    elif args.subparser_name == 'links':
        name = args.name[0].strip()
        if name.lower() == 'all':
            SiteList().links_all()
        else:
            Site(name).links()
    elif args.subparser_name == 'print':
        name = args.name[0].strip()
        if name.lower() == 'all':
            SiteList().print_all()
        else:
            Site(name).print()
    elif args.subparser_name == 'list':
        SiteList().list()
