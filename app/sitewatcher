#!/usr/bin/python3

import difflib
import hashlib
import logging
import os
import re
import requests
import sys
import time
from redis import Redis
from bs4 import BeautifulSoup
from urllib.parse import urljoin

logger = logging.getLogger()
logger.setLevel(logging.DEBUG)

redis_host = os.environ.get('REDIS_HOST', 'localhost')
redis_port = os.environ.get('REDIS_PORT', '6379')
redis_ttl = os.environ.get('REDIS_TTL', '60')
redis = Redis(host=redis_host, port=redis_port, decode_responses=True)

redis_skey_index = 'index'
redis_skey_hashes = 'hashes'
redis_skey_latests = 'latests'

redis_hkey_name = 'name'
redis_hkey_url = 'url'
redis_hkey_depth = 'depth'
redis_hkey_link = 'link'
redis_hkey_tag = 'tag'
redis_hkey_site = 'site'

def add_redis_name(name, resid):
    name_lower = name.lower()
    redis.set(name_lower, resid)
    redis.hset(resid, redis_hkey_name, name)
    redis.sadd(redis_skey_index, name_lower)

def delete_redis_name(name, resid):
    name_lower = name.lower()
    redis.srem(redis_skey_index, name_lower)
    redis.delete(name_lower)
    redis.delete(resid)

def rename_redis_name(old_name, new_name, resid):
    old_name_lower = old_name.lower()
    new_name_lower = new_name.lower()
    redis.rename(old_name_lower, new_name_lower)
    redis.hset(resid, redis_hkey_name, new_name)
    redis.sadd(redis_skey_index, new_name_lower)
    redis.srem(redis_skey_index, old_name_lower)

def get_redis_names():
    return redis.smembers(redis_skey_index)

def get_redis_resid(name):
    name_lower = name.lower()
    return redis.get(name_lower)

def set_redis_value(resid, hkey, hvalue):
    redis.hset(resid, hkey, hvalue)

def get_redis_value(resid, hkey):
    return redis.hget(resid, hkey)

def delete_redis_value(resid, hkey):
    return redis.hdel(resid, hkey)

def add_redis_smember(resid, skey, svalue):
    redis.sadd(resid + '+' + skey, svalue)
    redis.sadd(skey, svalue)

def remove_redis_smember(resid, skey, svalue):
    redis.srem(resid + '+' + skey, svalue)

def flush_redis_smembers(resid, skey):
    redis.delete(resid + '+' + skey)

def get_redis_smembers(resid, skey):
    return redis.smembers(resid + '+' + skey)

class Site:

    def __init__(self, name):
        name = name
        self.name = name
        self.resid = get_redis_resid(self.name)
        if self.resid is None:
            self.resid = hashlib.md5(self.name.encode()).hexdigest()
            self.url = None
            self.depth = None
            self.hashes = None
            self.latests = None
            self.exists = False
        else:
            self.name = get_redis_value(self.resid, redis_hkey_name)
            self.url = get_redis_value(self.resid, redis_hkey_url)
            self.depth = int(get_redis_value(self.resid, redis_hkey_depth))
            self.hashes = list(get_redis_smembers(self.resid, redis_skey_hashes))
            self.latests = list(get_redis_smembers(self.resid, redis_skey_latests))
            self.exists = True

    def add(self, url, depth):
        if self.exists == True:
            logger.error('Already exists.')
            return False

        if url is None:
            logger.error('Invalid url')
            return False

        if depth < 1:
            logger.error('Invalid depth')
            return False

        add_redis_name(self.name, self.resid)
        set_redis_value(self.resid, redis_hkey_url, url)
        set_redis_value(self.resid, redis_hkey_depth, depth)
        self.exists = True

        return True

    def delete(self):
        if self.exists == False:
            logger.error('No such a site.')
            return False

        delete_redis_name(self.name, self.resid)
        self.exists = False
        return True

    def rename(self, new_name):
        if self.exists == False:
            logger.error('No such a site.')
            return False

        old_name = self.name
        self.name = new_name
        rename_redis_name(old_name, self.name, self.resid)
        return True

    def get_real_title(self, bs):
        s = None
        s_tag = bs.find('title')
        s_ogp = bs.find('meta', attrs={'property': 'og:title'})
        if s_ogp is not None:
            s = s_ogp.get('content')
        elif s_tag is not None:
            s = s_tag.get_text()

        if s is None or len(s) == 0:
            return None
        else:
            return s

    def get_title(self, title):
        real_title = title['name']
        url = title['link']
        res = None
        try:
            res = requests.get(url)
        except requests.exceptions.InvalidSchema as e:
            logger.debug(e)
        except requests.exceptions.RequestException as e:
            logger.warning('Failed to fetch {}'.format(url))
            logger.debug(e)
        except Exception as e:
            logger.warning('Failed to fetch {}'.format(url))
            logger.debug(e)

        if res is not None:
            if res.status_code >= 400:
                logger.warning('Failed to fetch {}. Status code={}'.format(url, res.status_code))
            else:
                ftype = filetype.guess(res.content)
                if ftype:
                    real_title = '[' + ftype.extension + ']' + real_title
                else:
                    enc = res.encoding if res.encoding != 'ISO-8859-1' else None
                    bs = BeautifulSoup(content, 'html.parser', from_encoding=enc)
                    real_title = get_real_title(bs)

        return real_title + (' ' if len(real_title) > 0 else '') + '---- ' + url

    def get_references(self, bs, url_base, ignored_links):
        links = {}
        for t in bs.find_all('a'):
            ref = t.get('href')
            if ref is not None:
                ref = ''.join(filter(lambda c: c >= ' ', ref))
                if url_base is not None:
                    ref = urljoin(url_base, ref)
                if ref.startswith('javascript:'):
                    pass
                elif ref.startswith('http://warp.da.ndl.go.jp/collections/'):
                    pass
                else:
                    cs = t.strings
                    if cs is not None:
                        title = re.sub('[ã€€ ]+', ' ', '::'.join(filter(lambda x: len(x) > 0, [s.strip() for s in cs])))
                    tag = title + (' ' if len(title) > 0 else '') + '---- ' + ref
                    hash = hashlib.md5((self.resid + tag).encode()).hexdigest()
                    if hash not in ignored_links:
                        links.update({hash: { 'site': self.resid, 'name': title, 'link': ref, 'tag': tag }})
        return links

    def make_link_set(self, url, depth, links):

        res = None
        try:
            res = requests.get(url)
        except requests.exceptions.RequestException as e:
            logger.warning('Failed to fetch {} {}'.format(self.name, url))
            logger.debug(e)
            return None
        except Exception as e:
            logger.warning('Failed to fetch {} {}'.format(self.name, url))
            logger.debug(e)
            return None

        if res.status_code >= 400:
            logger.warning('Failed to fetch {} {}. Status code={}'.format(self.name, url, res.status_code))
            return None

        bs = BeautifulSoup(res.content, 'html.parser')
        child_links = self.get_references(bs, url, links)
        links.update(child_links)

        if depth > 1:
            for l in child_links:
                descendant_links = self.make_link_set(child_links[l]['link'], depth - 1, links)
            links.update(descendant_links)

        return links

    def update(self):
        if self.exists == False:
            logger.error('No such a site.')
            return False

        logger.info('Updating {}'.format(self.name))

        links = self.make_link_set(self.url, self.depth, {})
        if links is not None:
            hashes = links.keys()

            if self.hashes is None:
                latests = hashes
                obsoletes = []
            else:
                latests = list(set(hashes) - set(self.hashes))
                obsoletes = list(set(self.hashes) - set(hashes))

            print('old: {}'.format(self.hashes))
            print('new: {}'.format(hashes))
            print('latests: {}'.format(latests))
            print('obsoletes: {}'.format(obsoletes))

            if len(latests) == 0 and len(obsoletes) == 0:
                logger.info('Not updated {}'.format(self.name))
            else:
                if len(latests) > 0:
                    flush_redis_smembers(self.resid, redis_skey_latests)

                    for h in latests:
                        add_redis_smember(self.resid, redis_skey_hashes, h)
                        set_redis_value(h, redis_hkey_site, links[h]['site'])
                        set_redis_value(h, redis_hkey_name, links[h]['name'])
                        set_redis_value(h, redis_hkey_link, links[h]['link'])
                        set_redis_value(h, redis_hkey_tag, links[h]['tag'])
                        add_redis_smember(self.resid, redis_skey_latests, h)
                        print('added: {}: {}'.format(h, links[h]))

                for h in obsoletes:
                    remove_redis_smember(self.resid, redis_skey_hashes, h)
                    delete_redis_value(h, redis_hkey_site, links[h]['site'])
                    delete_redis_value(h, redis_hkey_name, links[h]['name'])
                    delete_redis_value(h, redis_hkey_link, links[h]['link'])
                    delete_redis_value(h, redis_hkey_tag, links[h]['tag'])
                    print('removed: {}: {}'.format(h, links[h]))

                logger.info('Updated {}'.format(self.name))

        return True

    def links(self):
        if self.exists == False:
            logger.error('No such a site.')
            return False

        if self.hashes is not None:
            for h in self.hashes:
                tag = get_redis_value(h, redis_hkey_tag)
                print("{} {}".format(h, tag))

    def latest(self):
        if self.exists == False:
            logger.error('No such a site.')
            return False

        if self.latests is not None:
            for h in self.latests:
                tag = get_redis_value(h, redis_hkey_tag)
                print("{} {}".format(h, tag))

class SiteList:

    def __init__(self):
        self.site_name_list = sorted(list(get_redis_names()))

    def list(self):
        for s in self.site_name_list:
            resid = get_redis_resid(s)
            name = get_redis_value(resid, redis_hkey_name)
            url = get_redis_value(resid, redis_hkey_url)
            depth = get_redis_value(resid, redis_hkey_depth)
            print('{} {} {} {}'.format(resid, name, url, depth))

    def update_all(self):
        for s in self.site_name_list:
            resid = get_redis_resid(s)
            name = get_redis_value(resid, redis_hkey_name)
            Site(name).update()

if __name__ == '__main__':

    import argparse
    import csv
    from argparse import HelpFormatter
    from operator import attrgetter
    class SortingHelpFormatter(HelpFormatter):
        def add_arguments(self, actions):
            actions = sorted(actions, key=attrgetter('option_strings'))
            super(SortingHelpFormatter, self).add_arguments(actions)

    parser = argparse.ArgumentParser(description='Check updating of web sites', formatter_class=SortingHelpFormatter)
    parser.add_argument('--debug', action='store_true', help='debug output')

    sps = parser.add_subparsers(dest='subparser_name', title='action arguments')
    sp_add = sps.add_parser('add', help='add a site')
    sp_add.add_argument('name', nargs=1, metavar='NAME', help='site name')
    sp_add.add_argument('url', nargs=1, metavar='URL', help='site url')
    sp_add.add_argument('depth', nargs='?', default='1', metavar='DEPTH', help='depth')
    sp_delete = sps.add_parser('delete', help='delete a site')
    sp_delete.add_argument('name', nargs=1, metavar='NAME', help='site name')
    sp_rename = sps.add_parser('rename', help='rename a site')
    sp_rename.add_argument('name', nargs=1, metavar='NAME', help='site name')
    sp_rename.add_argument('new_name', nargs=1, metavar='NEW_NAME', help='new name')
    sp_update = sps.add_parser('update', help='update a site or all sites')
    sp_update.add_argument('name', nargs=1, metavar='NAME', help='site name (\'all\' for all sites)')
    sp_links = sps.add_parser('links', help='show all links')
    sp_links.add_argument('name', nargs=1, metavar='NAME', help='site name')
    sp_latest = sps.add_parser('latest', help='show latest links')
    sp_latest.add_argument('name', nargs=1, metavar='NAME', help='site name')
    sp_list = sps.add_parser('list', help='list sites')

    if len(sys.argv) == 1:
        print(parser.format_usage(), file=sys.stderr)
        exit(0)

    args = parser.parse_args()

    log_format = '%(levelname)s: %(message)s'
    log_level = logging.INFO
    if args.debug is True:
        log_format = '%(asctime)s %(name)-12s %(levelname)-8s %(message)s'
        log_level = logging.DEBUG
    console = logging.StreamHandler()
    console.setFormatter(logging.Formatter(log_format))
    console.setLevel(log_level)
    logger.addHandler(console)

    if args.subparser_name == 'add':
        name = args.name[0].strip()
        url = args.url[0].strip()
        depth = int(args.depth)
        Site(name).add(url, depth)
    elif args.subparser_name == 'delete':
        name = args.name[0].strip()
        Site(name).delete()
    elif args.subparser_name == 'rename':
        name = args.name[0].strip()
        Site(name).rename(args.new_name[0])
    elif args.subparser_name == 'update':
        name = args.name[0].strip()
        if name.lower() == 'all':
            SiteList().update_all()
        else:
            Site(name).update()
    elif args.subparser_name == 'links':
        name = args.name[0].strip()
        Site(name).links()
    elif args.subparser_name == 'latest':
        name = args.name[0].strip()
        Site(name).latest()
    elif args.subparser_name == 'list':
        SiteList().list()
