#!/usr/bin/python3

import difflib
import filetype
import hashlib
import logging
import os
import re
import requests
import sys
import time
from bs4 import BeautifulSoup
from datetime import datetime
from redis import Redis
from urllib.parse import urljoin

debug_mode = False

logger = logging.getLogger()
logger.setLevel(logging.DEBUG)

redis_host = os.environ.get('REDIS_HOST', 'localhost')
redis_port = os.environ.get('REDIS_PORT', '6379')
redis_ttl = os.environ.get('REDIS_TTL', '60')
redis = Redis(host=redis_host, port=redis_port, decode_responses=True)

redis_skey_index = 'index'
redis_skey_hashes = 'hashes'
redis_skey_latests = 'latests'

redis_lkey_updated = 'updated'
redis_lmax_updated = 10

redis_hkey_name = 'name'
redis_hkey_url = 'url'
redis_hkey_depth = 'depth'
redis_hkey_link = 'link'
redis_hkey_parent = 'parent'
redis_hkey_tag = 'tag'
redis_hkey_site = 'site'

def add_redis_name(name, resid):
    name_lower = name.lower()
    redis.set(name_lower, resid)
    redis.hset(resid, redis_hkey_name, name)
    redis.sadd(redis_skey_index, name_lower)

def delete_redis_name(name, resid):
    name_lower = name.lower()
    redis.srem(redis_skey_index, name_lower)
    redis.delete(name_lower)
    redis.delete(resid)

def rename_redis_name(old_name, new_name, resid):
    old_name_lower = old_name.lower()
    new_name_lower = new_name.lower()
    redis.rename(old_name_lower, new_name_lower)
    redis.hset(resid, redis_hkey_name, new_name)
    redis.sadd(redis_skey_index, new_name_lower)
    redis.srem(redis_skey_index, old_name_lower)

def get_redis_names():
    return redis.smembers(redis_skey_index)

def get_redis_resid(name):
    name_lower = name.lower()
    return redis.get(name_lower)

def set_redis_value(resid, hkey, hvalue):
    redis.hset(resid, hkey, hvalue)

def get_redis_value(resid, hkey):
    return redis.hget(resid, hkey)

def delete_redis_value(resid, hkey):
    return redis.hdel(resid, hkey)

def add_redis_list_value(resid, lkey, lvalue, max_length=0, auto_remove=False):
    k = resid + '+' + lkey
    redis.lpush(k, lvalue)
    if max_length > 0 and redis.llen(k) > max_length:
        removed_value = redis.rpop(k)
        if auto_remove == True:
            redis.delete(resid + '+' + removed_value)

def get_redis_list_value(resid, lkey, index):
    return redis.lindex(resid + '+' + lkey, index)

def add_redis_smember(resid, skey, svalue):
    redis.sadd(resid + '+' + skey, svalue)
    redis.sadd(skey, svalue)

def remove_redis_smember(resid, skey, svalue):
    redis.srem(resid + '+' + skey, svalue)

def flush_redis_smembers(resid, skey):
    redis.delete(resid + '+' + skey)

def get_redis_smembers(resid, skey):
    return redis.smembers(resid + '+' + skey)

class Site:

    def __init__(self, name):
        self.name = name
        self.resid = get_redis_resid(self.name)
        if self.resid is None:
            self.resid = hashlib.md5(self.name.encode()).hexdigest()
            self.exists = False
        else:
            self.name = get_redis_value(self.resid, redis_hkey_name)
            self.exists = True

    def add(self, url, depth):
        if self.exists == True:
            logger.error('already exists')
            return False

        if url is None:
            logger.error('invalid url')
            return False

        if depth < 1:
            logger.error('invalid depth')
            return False

        add_redis_name(self.name, self.resid)
        set_redis_value(self.resid, redis_hkey_url, url)
        set_redis_value(self.resid, redis_hkey_depth, depth)
        self.exists = True

        return True

    def delete(self):
        if self.exists == False:
            logger.error('{}: no such a site'.format(self.name))
            return False

        delete_redis_name(self.name, self.resid)
        self.exists = False
        return True

    def rename(self, new_name):
        if self.exists == False:
            logger.error('{}: no such a site'.format(self.name))
            return False

        old_name = self.name
        self.name = new_name
        rename_redis_name(old_name, self.name, self.resid)
        return True

    def get_title(self, name, url, parent_name):
        title = name
        res = None
        try:
            res = requests.get(url)
        except requests.exceptions.InvalidSchema as e:
            logger.debug(e)
        except requests.exceptions.RequestException as e:
            logger.warning('{}: failed to fetch {}'.format(self.name, url))
            logger.debug(e)
        except Exception as e:
            logger.warning('{}: failed to fetch {}'.format(self.name, url))
            logger.debug(e)

        if res is not None:
            if res.status_code >= 400:
                logger.warning('{}: failed to fetch {}. Status code={}'.format(self.name, url, res.status_code))
            else:
                ftype = filetype.guess(res.content)
                if ftype:
                    if parent_name is None:
                        parent_name = ''
                    title = '[' + ftype.extension + ']' + parent_name + '::' + title
                    logger.debug('title: {} -> {}'.format(name, title))
                else:
                    real_title = None
                    enc = res.encoding if res.encoding != 'ISO-8859-1' else None
                    bs = BeautifulSoup(res.content, 'html.parser', from_encoding=enc)
                    bs_tag = bs.find('title')
                    bs_ogp = bs.find('meta', attrs={'property': 'og:title'})
                    if bs_ogp is not None:
                        real_title = bs_ogp.get('content')
                    elif bs_tag is not None:
                        real_title = bs_tag.get_text()
                    if real_title is not None and len(real_title) > 0:
                        title = title + '::' + real_title
                    logger.debug('title: {} -> {}'.format(name, title))

        return title

    def get_references(self, bs, parent_url, links, parent=None):
        links = {}
        for t in bs.find_all('a'):
            ref = t.get('href')
            if ref is not None:
                ref = ''.join(filter(lambda c: c >= ' ', ref))
                if parent_url is not None:
                    ref = urljoin(parent_url, ref)
                if ref.startswith('javascript:'):
                    pass
                elif ref.startswith('http://warp.da.ndl.go.jp/collections/'):
                    pass
                else:
                    cs = t.strings
                    if cs is not None:
                        title = re.sub('[ã€€ ]+', ' ', '::'.join(filter(lambda x: len(x) > 0, [s.strip() for s in cs])))
                    tag = title + (' ' if len(title) > 0 else '') + '---- ' + ref
                    hash = hashlib.md5((self.resid + tag).encode()).hexdigest()
                    if hash not in links:
                        links.update({hash: { 'site': self.resid, 'parent': parent, 'name': title, 'link': ref, 'tag': tag }})
        return links

    def make_link_set(self, url, depth, links, parent=None):

        res = None
        try:
            res = requests.get(url)
        except requests.exceptions.RequestException as e:
            logger.warning('{}: failed to fetch {}'.format(self.name, url))
            logger.debug(e)
            return None
        except Exception as e:
            logger.warning('{}: failed to fetch {}'.format(self.name, url))
            logger.debug(e)
            return None

        if res.status_code >= 400:
            logger.warning('{}: failed to fetch {}. Status code={}'.format(self.name, url, res.status_code))
            return None

        bs = BeautifulSoup(res.content, 'html.parser')
        children = self.get_references(bs, url, links, parent)
        links.update(children)

        if depth > 1:
            for h in children:
                descendant_links = self.make_link_set(children[h]['link'], depth - 1, links, h)
            links.update(descendant_links)

        return links

    def update(self, now=None):
        if self.exists == False:
            logger.error('{}: no such a site'.format(self.name))
            return False

        if now is None:
            now = time.time()

        url = get_redis_value(self.resid, redis_hkey_url)
        depth = int(get_redis_value(self.resid, redis_hkey_depth))
        links = self.make_link_set(url, depth, {})
        if links is None:
            logger.warning('{}: no links found'.format(self.name))
        else:
            hashes = links.keys()
            old_hashes = list(get_redis_smembers(self.resid, redis_skey_hashes))

            latests = list(set(hashes) - set(old_hashes))
            obsoletes = list(set(old_hashes) - set(hashes))

            logger.debug('{}: old: {}'.format(self.name, old_hashes))
            logger.debug('{}: new: {}'.format(self.name, hashes))
            logger.debug('{}: latests: {}'.format(self.name, latests))
            logger.debug('{}: obsoletes: {}'.format(self.name, obsoletes))

            if len(latests) > 0:
                for h in latests:
                    parent_name = None
                    parent = links[h]['parent']
                    if parent is not None:
                        parent_name = links[parent]['name']
                    title = self.get_title(links[h]['name'], links[h]['link'], parent_name)
                    if len(title) > 0:
                        links[h]['name'] = title
                        links[h]['tag'] = title + ' ---- ' + links[h]['link']
                    add_redis_smember(self.resid, redis_skey_hashes, h)
                    set_redis_value(h, redis_hkey_site, links[h]['site'])
                    set_redis_value(h, redis_hkey_name, links[h]['name'])
                    set_redis_value(h, redis_hkey_link, links[h]['link'])
                    if links[h]['parent'] is not None:
                        set_redis_value(h, redis_hkey_parent, links[h]['parent'])
                    set_redis_value(h, redis_hkey_tag, links[h]['tag'])
                    logger.debug('{}: added: {} {}'.format(self.name, h, links[h]['tag']))

                if len(old_hashes) == 0:
                    logger.debug('{}: first update'.format(self.name))
                else:
                    for h in latests:
                        add_redis_smember(self.resid, str(now), h)

                print('{}: updated'.format(self.name))

            for h in obsoletes:
                remove_redis_smember(self.resid, redis_skey_hashes, h)
                obsolete_tag = get_redis_value(h, redis_hkey_tag)
                delete_redis_value(h, redis_hkey_site)
                delete_redis_value(h, redis_hkey_name)
                delete_redis_value(h, redis_hkey_link)
                delete_redis_value(h, redis_hkey_parent)
                delete_redis_value(h, redis_hkey_tag)
                logger.debug('{}: removed: {} {}'.format(self.name, h, obsolete_tag))

        add_redis_list_value(self.resid, redis_lkey_updated, str(now), redis_lmax_updated, True)
        logger.debug('{}: updated: {}'.format(self.name, now))

        return True

    def show(self):
        if self.exists == False:
            logger.error('{}: no such a site'.format(self.name))
            return False

        url = get_redis_value(self.resid, redis_hkey_url)
        depth = int(get_redis_value(self.resid, redis_hkey_depth))
        print('Site: {} {}\n  URL: {}\n  Depth: {}'.format(self.resid, self.name, url, depth))

        hashes = list(get_redis_smembers(self.resid, redis_skey_hashes))
        for h in hashes:
            n = get_redis_value(h,redis_hkey_name)
            l = get_redis_value(h, redis_hkey_link)
            t = get_redis_value(h, redis_hkey_tag)
            print('Hash: {} {}\n  Link: {}\n  Tag: {}'.format(h, n, l, t))
            sh = get_redis_value(h, redis_hkey_site)
            if sh is not None:
                sn = get_redis_value(sh, redis_hkey_name)
                print('  Site: {} {}'.format(sh, sn))
            ph = get_redis_value(h, redis_hkey_parent)
            if ph is not None:
                pn = get_redis_value(ph, redis_hkey_name)
                print('  Parent: {} {}'.format(ph, pn))

        return True

    def links(self):
        if self.exists == False:
            logger.error('{}: no such a site'.format(self.name))
            return False

        hashes = list(get_redis_smembers(self.resid, redis_skey_hashes))
        if old_hashes is not None:
            if debug_mode is True:
                print('links: {}'.format(hashes))
            for h in hashes:
                tag = get_redis_value(h, redis_hkey_tag)
                if debug_mode is True:
                    print('{} {} {}'.format(h, self.name, tag))
                else:
                    print('{} {}'.format(self.name, tag))

    def sequences(self):
        if self.exists == False:
            logger.error('{}: no such a site'.format(self.name))
            return False

        for seq in range(1, redis_lmax_updated):
            updated = get_redis_list_value(self.resid, redis_lkey_updated, seq)
            if updated is not None:
                hashes = list(get_redis_smembers(self.resid, updated))
                if hashes is None:
                    population = 0
                else:
                    population = len(hashes)
                updated_isotimestamp = datetime.utcfromtimestamp(float(updated)).isoformat()
                print('{} {} {} {}'.format(seq, population, updated, updated_isotimestamp))

    def print(self, sequence):
        if self.exists == False:
            logger.error('{}: no such a site'.format(self.name))
            return False

        updated = get_redis_list_value(self.resid, redis_lkey_updated, sequence)
        if updated is not None:
            hashes = list(get_redis_smembers(self.resid, updated))
            if debug_mode is True:
                updated_isotimestamp = datetime.utcfromtimestamp(float(updated)).isoformat()
                print('updated: {} {}'.format(updated, updated_isotimestamp))
            if hashes is not None:
                if debug_mode is True:
                    print('hashes: {}'.format(hashes))
                for h in hashes:
                    tag = get_redis_value(h, redis_hkey_tag)
                    if debug_mode is True:
                        print('{} {} {}'.format(h, self.name, tag))
                    else:
                        print('{} {}'.format(self.name, tag))

class SiteList:

    def __init__(self):
        self.site_name_list = sorted(list(get_redis_names()))

    def list(self):
        for s in self.site_name_list:
            resid = get_redis_resid(s)
            name = get_redis_value(resid, redis_hkey_name)
            url = get_redis_value(resid, redis_hkey_url)
            depth = get_redis_value(resid, redis_hkey_depth)
            if debug_mode is True:
                print('{} {} {} {}'.format(resid, name, url, depth))
            else:
                print('{} {} {}'.format(name, url, depth))

    def update_all(self):
        now = time.time()
        for s in self.site_name_list:
            resid = get_redis_resid(s)
            name = get_redis_value(resid, redis_hkey_name)
            Site(name).update(now)

    def print_all(self, sequence):
        for s in self.site_name_list:
            resid = get_redis_resid(s)
            name = get_redis_value(resid, redis_hkey_name)
            Site(name).print(sequence)

    def links_all(self):
        for s in self.site_name_list:
            resid = get_redis_resid(s)
            name = get_redis_value(resid, redis_hkey_name)
            Site(name).links()

    def sequences_all(self):
        for s in self.site_name_list:
            resid = get_redis_resid(s)
            name = get_redis_value(resid, redis_hkey_name)
            Site(name).sequences()

if __name__ == '__main__':

    import argparse
    import csv
    from argparse import HelpFormatter
    from operator import attrgetter
    class SortingHelpFormatter(HelpFormatter):
        def add_arguments(self, actions):
            actions = sorted(actions, key=attrgetter('option_strings'))
            super(SortingHelpFormatter, self).add_arguments(actions)

    parser = argparse.ArgumentParser(description='Check updating of web sites', formatter_class=SortingHelpFormatter)
    parser.add_argument('--debug', action='store_true', help='debug output')

    sps = parser.add_subparsers(dest='subparser_name', title='action arguments')
    sp_add = sps.add_parser('add', help='add a site')
    sp_add.add_argument('name', nargs=1, metavar='NAME', help='site name')
    sp_add.add_argument('url', nargs=1, metavar='URL', help='site url')
    sp_add.add_argument('depth', nargs='?', default='1', metavar='DEPTH', help='depth')
    sp_delete = sps.add_parser('delete', help='delete a site')
    sp_delete.add_argument('name', nargs=1, metavar='NAME', help='site name')
    sp_rename = sps.add_parser('rename', help='rename a site')
    sp_rename.add_argument('name', nargs=1, metavar='NAME', help='site name')
    sp_rename.add_argument('new_name', nargs=1, metavar='NEW_NAME', help='new name')
    sp_show = sps.add_parser('show', help='show a site related data')
    sp_show.add_argument('name', nargs=1, metavar='NAME', help='site name')
    sp_update = sps.add_parser('update', help='update a site or all sites')
    sp_update.add_argument('name', nargs=1, metavar='NAME', help='site name (\'all\' for all sites)')
    sp_links = sps.add_parser('links', help='print all links of a site or all sites')
    sp_links.add_argument('name', nargs=1, metavar='NAME', help='site name (\'all\' for all sites)')
    sp_print = sps.add_parser('print', help='print latest links of a site or all sites')
    sp_print.add_argument('name', nargs=1, metavar='NAME', help='site name (\'all\' for all sites)')
    sp_print.add_argument('--sequence', '-s', default='0', metavar='N', help='time sequence number (latest=0)')
    sp_sequences = sps.add_parser('sequences', help='list time sequences')
    sp_sequences.add_argument('name', nargs=1, metavar='NAME', help='site name')
    sp_list = sps.add_parser('list', help='list sites')

    if len(sys.argv) == 1:
        print(parser.format_usage(), file=sys.stderr)
        exit(0)

    args = parser.parse_args()
    debug_mode = args.debug

    log_format = '%(levelname)s: %(message)s'
    log_level = logging.INFO
    if debug_mode is True:
        log_format = '%(asctime)s %(name)-12s %(levelname)-8s %(message)s'
        log_level = logging.DEBUG
    console = logging.StreamHandler()
    console.setFormatter(logging.Formatter(log_format))
    console.setLevel(log_level)
    logger.addHandler(console)

    if args.subparser_name == 'add':
        name = args.name[0].strip()
        url = args.url[0].strip()
        depth = int(args.depth)
        Site(name).add(url, depth)
    elif args.subparser_name == 'delete':
        name = args.name[0].strip()
        Site(name).delete()
    elif args.subparser_name == 'rename':
        name = args.name[0].strip()
        Site(name).rename(args.new_name[0])
    elif args.subparser_name == 'show':
        name = args.name[0].strip()
        Site(name).show()
    elif args.subparser_name == 'update':
        name = args.name[0].strip()
        if name.lower() == 'all':
            SiteList().update_all()
        else:
            Site(name).update()
    elif args.subparser_name == 'links':
        name = args.name[0].strip()
        if name.lower() == 'all':
            SiteList().links_all()
        else:
            Site(name).links()
    elif args.subparser_name == 'print':
        name = args.name[0].strip()
        seq = args.sequence
        if name.lower() == 'all':
            SiteList().print_all(seq)
        else:
            Site(name).print(seq)
    elif args.subparser_name == 'sequences':
        name = args.name[0].strip()
        if name.lower() == 'all':
            SiteList().sequences_all()
        else:
            Site(name).sequences()
    elif args.subparser_name == 'list':
        SiteList().list()
