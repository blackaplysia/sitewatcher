#!/usr/bin/python3

import difflib
import filetype
import hashlib
import json
import logging
import os
import re
import requests
import sys
import time
from bs4 import BeautifulSoup
from datetime import datetime
from redis import Redis
from urllib.parse import urljoin

from ifprint import PrintInterface

debug_mode = False

logger = logging.getLogger()
logger.setLevel(logging.DEBUG)

redis_host = os.environ.get('REDIS_HOST', 'localhost')
redis_port = os.environ.get('REDIS_PORT', '6379')
redis_ttl = os.environ.get('REDIS_TTL', '60')
redis = Redis(host=redis_host, port=redis_port, decode_responses=True)

json_section_data = 'data'
json_section_config = 'config'
json_section_header = 'header'
json_section_updated = 'updated'
json_section_links = 'links'

redis_skey_index = 'index'
redis_skey_ignores = 'ignores'
redis_skey_hashes = 'hashes'
redis_skey_latests = 'latests'

redis_lkey_updated = 'updated'
redis_lmax_updated = 10

redis_strkey_resid = 'resid'

redis_hkey_name = 'name'
redis_hkey_depth = 'depth'
redis_hkey_link = 'link'
redis_hkey_parent = 'parent'
redis_hkey_tag = 'tag'
redis_hkey_site = 'site'

def is_redis_empty():
    return redis.dbsize() == 0

def add_redis_name(name, resid):
    name_lower = name.lower()
    redis.set(name_lower, resid)
    redis.hset(resid, redis_hkey_name, name)
    redis.sadd(redis_skey_index, name_lower)

def delete_redis_name(name, resid):
    name_lower = name.lower()
    redis.srem(redis_skey_index, name_lower)
    redis.delete(name_lower)
    redis.delete(resid)

def rename_redis_name(old_name, new_name, resid):
    old_name_lower = old_name.lower()
    new_name_lower = new_name.lower()
    redis.rename(old_name_lower, new_name_lower)
    redis.hset(resid, redis_hkey_name, new_name)
    redis.sadd(redis_skey_index, new_name_lower)
    redis.srem(redis_skey_index, old_name_lower)

def get_redis_names():
    return redis.smembers(redis_skey_index)

def get_redis_resid(name):
    name_lower = name.lower()
    return redis.get(name_lower)

def add_redis_ignores(link):
    redis.sadd(redis_skey_ignores, link)

def remove_redis_ignores(link):
    redis.srem(redis_skey_ignores, link)

def get_redis_ignores():
    return redis.smembers(redis_skey_ignores)

def set_redis_value(resid, hkey, hvalue):
    redis.hset(resid, hkey, hvalue)

def get_redis_value(resid, hkey):
    return redis.hget(resid, hkey)

def delete_redis_value(resid, hkey):
    redis.hdel(resid, hkey)

def delete_redis_values(resid):
    redis.delete(resid)

def add_redis_list_value(resid, lkey, lvalue, max_length=0, auto_remove=False):
    k = resid + '+' + lkey
    redis.lpush(k, lvalue)
    if max_length > 0 and redis.llen(k) > max_length:
        removed_value = redis.rpop(k)
        if auto_remove == True:
            redis.delete(resid + '+' + removed_value)

def get_redis_list_value(resid, lkey, index):
    return redis.lindex(resid + '+' + lkey, index)

def get_redis_list_values(resid, lkey):
    return redis.lrange(resid + '+' + lkey, 0, -1)

def delete_redis_list(resid, lkey):
    redis.delete(resid + '+' + lkey)

def add_redis_smember(resid, skey, svalue):
    redis.sadd(resid + '+' + skey, svalue)
    # redis.sadd(skey, svalue)

def remove_redis_smember(resid, skey, svalue):
    redis.srem(resid + '+' + skey, svalue)

def flush_redis_smembers(resid, skey):
    redis.delete(resid + '+' + skey)

def get_redis_smembers(resid, skey):
    return redis.smembers(resid + '+' + skey)

def delete_redis_set(resid, skey):
    redis.delete(resid + '+' + skey)

def dump_redis_data():
    index = redis.smembers(redis_skey_index)
    if index is None:
        return None

    global_config = {}
    ignores = redis.smembers(redis_skey_ignores)
    if ignores is not None:
        global_config.update({ redis_skey_ignores: list(ignores) })

    global_data = {}
    for n in index:
        resid = redis.get(n)
        name = redis.hget(resid, redis_hkey_name)
        link = redis.hget(resid, redis_hkey_link)
        depth = redis.hget(resid, redis_hkey_depth)

        updated = []
        updated_date_list = redis.lrange(resid + '+' + redis_lkey_updated, 0, -1)
        for dt in updated_date_list:
            updated.append({ dt: list(redis.smembers(resid + '+' + dt))})

        config = {}
        ignores = redis.smembers(resid + '+' + redis_skey_ignores)
        if ignores is not None:
            config.update({ redis_skey_ignores: list(ignores) })

        links = {}
        hashes = redis.smembers(resid + '+' + redis_skey_hashes)
        for h in hashes:
            hn = redis.hget(h, redis_hkey_name)
            hl = redis.hget(h, redis_hkey_link)
            hp = redis.hget(h, redis_hkey_parent)
            hs = redis.hget(h, redis_hkey_site)
            ht = redis.hget(h, redis_hkey_tag)
            links.update({
                h: {
                    redis_hkey_name: hn,
                    redis_hkey_link: hl,
                    redis_hkey_parent: hp,
                    redis_hkey_site: hs,
                    redis_hkey_tag: ht
                }
            })

        global_data.update({
            n: {
                json_section_header: {
                    redis_strkey_resid: resid,
                    redis_hkey_name: name,
                    redis_hkey_link: link,
                    redis_hkey_depth: depth,
                },
                json_section_config: config,
                json_section_updated: updated,
                json_section_links: links
            }
        })

        logger.info('{}: {} links and {} sequences dumped'.format(name, len(hashes), len(updated_date_list)))

    json = {
        json_section_config: global_config,
        json_section_data: global_data
    }

    return json

def load_redis_data(json):
    global_config = json[json_section_config]
    global_data = json[json_section_data]

    ignores = global_config[redis_skey_ignores]
    for i in ignores:
        redis.sadd(redis_skey_ignores, i)

    for site in global_data:
        header = global_data[site][json_section_header]
        resid = header[redis_strkey_resid]

        redis.set(site, resid)
        redis.hset(resid, redis_hkey_name, header[redis_hkey_name])
        redis.sadd(redis_skey_index, site)
        redis.hset(resid, redis_hkey_link, header[redis_hkey_link])
        redis.hset(resid, redis_hkey_depth, header[redis_hkey_depth])
        print('set {}, {}'.format(site, resid))
        print('hset {}, {}, {}'.format(resid, redis_hkey_name, site))
        print('sadd {}, {}'.format(redis_skey_index, site))
        print('hset {}, {}, {}'.format(resid, redis_hkey_link, header[redis_hkey_link]))
        print('hset {}, {}, {}'.format(resid, redis_hkey_depth, header[redis_hkey_depth]))

        config = global_data[site][json_section_config]
        ignores = config[redis_skey_ignores]
        for i in ignores:
            redis.sadd(resid + '+' + redis_skey_ignores, i)

        updated = global_data[site][json_section_updated]
        for sequence in updated:
            dt = list(sequence)[0]
            redis.rpush(resid + '+' + redis_lkey_updated, dt)
            print('rpush {}, {}'.format(resid + '+' + redis_lkey_updated, dt))
            for uh in sequence[dt]:
                redis.sadd(resid + '+' + dt, uh)
                #redis.sadd(dt, uh)
                print('sadd {}, {}'.format(resid + '+' + dt, uh))
                #print('sadd {}, {}'.format(dt, uh))

        links = global_data[site][json_section_links]
        for h in links:
            redis.sadd(resid + '+' + redis_skey_hashes, h)
            redis.hset(h, redis_hkey_name, links[h][redis_hkey_name])
            redis.hset(h, redis_hkey_link, links[h][redis_hkey_link])
            if links[h][redis_hkey_parent] is not None:
                redis.hset(h, redis_hkey_parent, links[h][redis_hkey_parent])
            redis.hset(h, redis_hkey_site, links[h][redis_hkey_site])
            redis.hset(h, redis_hkey_tag, links[h][redis_hkey_tag])
            print('hadd {}, {}'.format(resid + '+' + redis_skey_hashes, h))
            print('hset {}, {}, {}'.format(h, redis_hkey_name, links[h][redis_hkey_name]))
            print('hset {}, {}, {}'.format(h, redis_hkey_link, links[h][redis_hkey_link]))
            print('hset {}, {}, {}'.format(h, redis_hkey_parent, links[h][redis_hkey_parent]))
            print('hset {}, {}, {}'.format(h, redis_hkey_site, links[h][redis_hkey_site]))
            print('hset {}, {}, {}'.format(h, redis_hkey_tag, links[h][redis_hkey_tag]))

    return len(global_data)

class Site:

    def __init__(self, name):
        self.name = name
        self.resid = get_redis_resid(self.name)
        if self.resid is None:
            self.resid = hashlib.md5(self.name.encode()).hexdigest()
            self.exists = False
        else:
            self.name = get_redis_value(self.resid, redis_hkey_name)
            self.exists = True

    def add(self, link, depth):
        if self.exists == True:
            logger.error('already exists')
            return False

        if link is None:
            logger.error('invalid url')
            return False

        if depth < 1:
            logger.error('invalid depth')
            return False

        add_redis_name(self.name, self.resid)
        set_redis_value(self.resid, redis_hkey_link, link)
        set_redis_value(self.resid, redis_hkey_depth, depth)
        self.exists = True

        return True

    def delete(self):
        if self.exists == False:
            logger.error('{}: no such a site'.format(self.name))
            return False

        for updated in get_redis_list_values(self.resid, redis_lkey_updated):
            delete_redis_set(self.resid, updated)
            print('deleted {}'.format(self.resid + '+' + updated))
        delete_redis_list(self.resid, redis_lkey_updated)
        print('deleted {}'.format(self.resid + '+' + redis_lkey_updated))

        hashes = get_redis_smembers(self.resid, redis_skey_hashes)
        for h in hashes:
            delete_redis_values(h)
            print('deleted {}'.format(h))
        delete_redis_set(self.resid, redis_skey_hashes)
        print('deleted {}'.format(self.resid + '+' + redis_skey_hashes))
        
        delete_redis_set(self.resid, redis_skey_ignores)
        print('deleted {}'.format(self.resid + '+' + redis_skey_ignores))

        delete_redis_name(self.name, self.resid)
        print('deleted {}'.format(self.name))
        print('deleted {}'.format(self.resid))
        print('index {}'.format(get_redis_names()))
        self.exists = False
        return True

    def rename(self, new_name):
        if self.exists == False:
            logger.error('{}: no such a site'.format(self.name))
            return False

        old_name = self.name
        self.name = new_name
        rename_redis_name(old_name, self.name, self.resid)
        return True

    def config(self, linkv=None, depthv=None, ignoresv=None, recognizev=None):
        if self.exists == False:
            logger.error('{}: no such a site'.format(self.name))
            return False

        if linkv is not None:
            set_redis_value(self.resid, redis_hkey_link, linkv[0])
        if depthv is not None:
            set_redis_value(self.resid, redis_hkey_depth, depthv[0])
        if ignoresv is not None:
            for i in ignoresv:
                add_redis_smember(self.resid, redis_skey_ignores, i)
        if recognizev is not None:
            for r in recognizev:
                remove_redis_smember(self.resid, redis_skey_ignores, r)

        link = get_redis_value(self.resid, redis_hkey_link)
        depth = get_redis_value(self.resid, redis_hkey_depth)
        ignores = get_redis_smembers(self.resid, redis_skey_ignores)
        if debug_mode is True:
            print('{} {} {} {}'.format(self.resid, self.name, link, depth))
            for i in ignores:
                print('{} {} ignores {}'.format(self.resid, self.name, i))
        else:
            print('{} {} {}'.format(self.name, link, depth))
            for i in ignores:
                print('{} ignores {}'.format(self.name, i))

        return True

    def get_title(self, name, link, parent_name):
        title = name
        res = None
        try:
            res = requests.get(link)
        except requests.exceptions.InvalidSchema as e:
            logger.debug(e)
        except requests.exceptions.RequestException as e:
            logger.warning('{}: failed to fetch {}'.format(self.name, link))
            logger.debug(e)
        except Exception as e:
            logger.warning('{}: failed to fetch {}'.format(self.name, link))
            logger.debug(e)

        if res is not None:
            if res.status_code >= 400:
                logger.warning('{}: failed to fetch {}. Status code={}'.format(self.name, link, res.status_code))
            else:
                ftype = filetype.guess(res.content)
                if ftype:
                    if parent_name is None:
                        parent_name = ''
                    title = '[' + ftype.extension + ']' + parent_name + '::' + title
                    logger.debug('title: {} -> {}'.format(name, title))
                else:
                    real_title = None
                    enc = res.encoding if res.encoding != 'ISO-8859-1' else None
                    bs = BeautifulSoup(res.content, 'html.parser', from_encoding=enc)
                    bs_tag = bs.find('title')
                    bs_ogp = bs.find('meta', attrs={'property': 'og:title'})
                    if bs_ogp is not None:
                        real_title = bs_ogp.get('content')
                    elif bs_tag is not None:
                        real_title = bs_tag.get_text()
                    if real_title is not None:
                        real_title = real_title.strip()
                        lt = len(title)
                        lrt = len(real_title)
                        if lrt > 0:
                            if lt >= lrt and title.find(real_title) >= 0:
                                pass
                            elif lt <= lrt and real_title.find(title) >= 0:
                                title = real_title
                            else:
                                title = title + '::' + real_title.strip()
                    logger.debug('title: {} -> {}'.format(name, title))

        return title

    def get_references(self, bs, parent_hash, parent_link, links, ignores):
        children = {}
        for t in bs.find_all('a'):
            ref = t.get('href')
            if ref is not None:
                ref = ''.join(filter(lambda c: c >= ' ', ref))
                ref = re.sub('<.*?>', '', ref)
                ref = ref.strip()

                ref_lower = ref.lower()
                if ref_lower.startswith('#'):
                    pass
                elif ref_lower.startswith('mailto:'):
                    pass
                elif ref_lower.startswith('tel:'):
                    pass
                elif ref_lower.startswith('javascript:'):
                    pass
                else:
                    if parent_link is not None:
                        ref = urljoin(parent_link, ref)
                        ref_lower = ref.lower()
                    is_ignored = False
                    if ignores is not None:
                        for i in ignores:
                            if ref_lower.startswith(i):
                                is_ignored = True
                                logger.debug('{} ignores {} which matches {}'.format(self.name, ref_lower, i))
                                break
                    if is_ignored is False:
                        cs = t.strings
                        if cs is not None:
                            title = re.sub('[　 ]+', ' ', '::'.join(filter(lambda x: len(x) > 0, [s.strip() for s in cs])))
                        tag = title + (' ' if len(title) > 0 else '') + '---- ' + ref
                        hash = hashlib.md5((self.resid + tag).encode()).hexdigest()
                        if hash not in links:
                            children.update({hash: { 'site': self.resid, 'parent': parent_hash, 'name': title, 'link': ref, 'tag': tag }})
        return children

    def make_link_set(self, hash, link, depth, links, ignores):

        res = None
        try:
            res = requests.get(link)
        except requests.exceptions.RequestException as e:
            logger.warning('{}: failed to fetch {}'.format(self.name, link))
            logger.debug(e)
            return None
        except Exception as e:
            logger.warning('{}: failed to fetch {}'.format(self.name, link))
            logger.debug(e)
            return None

        if res.status_code >= 400:
            logger.warning('{}: failed to fetch {}. Status code={}'.format(self.name, link, res.status_code))
            return None

        bs = BeautifulSoup(res.content, 'html.parser')
        children = self.get_references(bs, hash, link, links, ignores)
        links.update(children)

        if depth > 1:
            for h in children:
                descendant_links = self.make_link_set(h, children[h]['link'], depth - 1, links, ignores)
                if descendant_links is not None:
                    links.update(descendant_links)

        return links

    def update(self, now=None):
        if self.exists == False:
            logger.error('{}: no such a site'.format(self.name))
            return False

        if now is None:
            now = time.time()

        link = get_redis_value(self.resid, redis_hkey_link)
        depth = int(get_redis_value(self.resid, redis_hkey_depth))
        old_hashes = list(get_redis_smembers(self.resid, redis_skey_hashes))
        if len(old_hashes) == 0:
            logger.info('{}: first update'.format(self.name))
        ignores = get_redis_smembers(self.resid, redis_skey_ignores)
        ignores_all = get_redis_ignores()
        if ignores_all is not None:
            if ignores is None:
                ignores = ignores_all
            else:
                ignores.update(ignores_all)

        links = {}
        links = self.make_link_set(self.resid, link, depth, links, ignores)
        if links is None:
            logger.warning('{}: no links found'.format(self.name))
        else:
            hashes = links.keys()

            latests = list(set(hashes) - set(old_hashes))
            obsoletes = list(set(old_hashes) - set(hashes))

            logger.debug('{}: old: {}'.format(self.name, old_hashes))
            logger.debug('{}: new: {}'.format(self.name, hashes))
            logger.debug('{}: latests: {}'.format(self.name, latests))
            logger.debug('{}: obsoletes: {}'.format(self.name, obsoletes))

            if len(latests) > 0:
                for h in latests:
                    parent_name = None
                    parent = links[h]['parent']
                    if parent is not None:
                        if parent not in links:
                            parent_name = self.name
                        else:
                            parent_name = links[parent]['name']
                    title = self.get_title(links[h]['name'], links[h]['link'], parent_name)
                    if len(title) > 0:
                        links[h]['name'] = title
                        links[h]['tag'] = title + ' ---- ' + links[h]['link']
                    add_redis_smember(self.resid, redis_skey_hashes, h)
                    set_redis_value(h, redis_hkey_site, links[h]['site'])
                    set_redis_value(h, redis_hkey_name, links[h]['name'])
                    set_redis_value(h, redis_hkey_link, links[h]['link'])
                    if links[h]['parent'] is not None:
                        set_redis_value(h, redis_hkey_parent, links[h]['parent'])
                    set_redis_value(h, redis_hkey_tag, links[h]['tag'])
                    logger.debug('{}: added: {} {}'.format(self.name, h, links[h]['tag']))

                if len(old_hashes) > 0:
                    for h in latests:
                        add_redis_smember(self.resid, str(now), h)

                print('{}: updated'.format(self.name))

            for h in obsoletes:
                remove_redis_smember(self.resid, redis_skey_hashes, h)
                obsolete_tag = get_redis_value(h, redis_hkey_tag)
                delete_redis_value(h, redis_hkey_site)
                delete_redis_value(h, redis_hkey_name)
                delete_redis_value(h, redis_hkey_link)
                delete_redis_value(h, redis_hkey_parent)
                delete_redis_value(h, redis_hkey_tag)
                logger.debug('{}: removed: {} {}'.format(self.name, h, obsolete_tag))

        add_redis_list_value(self.resid, redis_lkey_updated, str(now), redis_lmax_updated, True)
        logger.debug('{}: updated: {}'.format(self.name, now))

        return True

    def links(self, sequence):
        if self.exists == False:
            logger.error('{}: no such a site'.format(self.name))
            return False

        if sequence is None:
            hashes = list(get_redis_smembers(self.resid, redis_skey_hashes))
        else:
            updated = get_redis_list_value(self.resid, redis_lkey_updated, sequence)
            if updated is not None:
                hashes = list(get_redis_smembers(self.resid, updated))
                if hashes is not None:
                    if debug_mode is True:
                        print('hashes: {}'.format(hashes))
                    for h in hashes:
                        n = get_redis_value(h,redis_hkey_name)
                        l = get_redis_value(h, redis_hkey_link)
                        t = get_redis_value(h, redis_hkey_tag)
                        ph = get_redis_value(h, redis_hkey_parent)
                        if ph is None:
                            pl = None
                        else:
                            pl = get_redis_value(ph, redis_hkey_link)
                        sh = get_redis_value(h, redis_hkey_site)
                        if sh is None:
                            sl = None
                        else:
                            sl = get_redis_value(sh, redis_hkey_link)
                        if debug_mode:
                            print('{} {} {} {} {} {} {} {}'.format(self.name, sh, ph, h, sl, pl, l, n))
                        else:
                            print('{} {} {} {} {}'.format(self.name, sl, pl, l, n))

        return True

    def sequences(self):
        if self.exists == False:
            logger.error('{}: no such a site'.format(self.name))
            return False

        for seq in range(0, redis_lmax_updated):
            updated = get_redis_list_value(self.resid, redis_lkey_updated, seq)
            if updated is not None:
                hashes = list(get_redis_smembers(self.resid, updated))
                if hashes is None:
                    population = 0
                else:
                    population = len(hashes)
                updated_isotimestamp = datetime.utcfromtimestamp(float(updated)).isoformat()
                print('{} {} {} {}'.format(seq, population, updated, updated_isotimestamp))

    def print(self, sequence, interface, channel=None):
        if self.exists == False:
            logger.error('{}: no such a site'.format(self.name))
            return False

        updated = get_redis_list_value(self.resid, redis_lkey_updated, sequence)

        if updated is not None:
            hashes = list(get_redis_smembers(self.resid, updated))
            if debug_mode is True:
                updated_isotimestamp = datetime.utcfromtimestamp(float(updated)).isoformat()
                print('updated: {} {}'.format(updated, updated_isotimestamp))
            if hashes is not None:
                if debug_mode is True:
                    print('hashes: {}'.format(hashes))
                for h in hashes:
                    tag = get_redis_value(h, redis_hkey_tag)
                    if tag is None:
                        tag = 'obsolete'
                    if debug_mode is True:
                        interface.print(self.name, tag, channel, h)
                    else:
                        interface.print(self.name, tag, channel)

class SiteList:

    def __init__(self):
        self.site_name_list = sorted(list(get_redis_names()))

    def list(self):
        for s in self.site_name_list:
            resid = get_redis_resid(s)
            name = get_redis_value(resid, redis_hkey_name)
            link = get_redis_value(resid, redis_hkey_link)
            depth = get_redis_value(resid, redis_hkey_depth)
            if debug_mode is True:
                print('{} {} {} {}'.format(resid, name, link, depth))
            else:
                print('{} {} {}'.format(name, link, depth))

    def update_all(self):
        now = time.time()
        for s in self.site_name_list:
            resid = get_redis_resid(s)
            name = get_redis_value(resid, redis_hkey_name)
            Site(name).update(now)

    def print_all(self, sequence, interface, channel=None):
        for s in self.site_name_list:
            resid = get_redis_resid(s)
            name = get_redis_value(resid, redis_hkey_name)
            Site(name).print(sequence, interface, channel)

    def links_all(self, sequence):
        for s in self.site_name_list:
            resid = get_redis_resid(s)
            name = get_redis_value(resid, redis_hkey_name)
            Site(name).links(sequence)

    def sequences_all(self):
        for s in self.site_name_list:
            resid = get_redis_resid(s)
            name = get_redis_value(resid, redis_hkey_name)
            Site(name).sequences()

    def config(self, linkv, depthv, ignoresv, recognizev):
        if linkv is not None or depthv is not None:
            logger.error('cannot apply to all sites')
            return

        if ignoresv is not None:
            for i in ignoresv:
                add_redis_ignores(i)
        if recognizev is not None:
            for r in recognizev:
                remove_redis_ignores(r)

        for i in get_redis_ignores():
            print('ignores {}'.format(i))

    def export_data(self):
        data = dump_redis_data()
        if data is None:
            logger.info('no data')
        else:
            print(json.dumps(data, ensure_ascii=False, indent=1))

    def import_data(self):
        if is_redis_empty() == False:
            logger.error('database is not empty')
        else:
            data = json.loads(sys.stdin.read())
            if data is None:
                logger.error('no data')
            else:
                count = load_redis_data(data)
                logger.info('{} sites imported'.format(count))

if __name__ == '__main__':

    import argparse
    import csv
    from argparse import HelpFormatter
    from operator import attrgetter
    class SortingHelpFormatter(HelpFormatter):
        def add_arguments(self, actions):
            actions = sorted(actions, key=attrgetter('option_strings'))
            super(SortingHelpFormatter, self).add_arguments(actions)

    parser = argparse.ArgumentParser(description='Check updating of web sites', formatter_class=SortingHelpFormatter)
    parser.add_argument('--debug', action='store_true', help='debug output')

    sps = parser.add_subparsers(dest='subparser_name', title='action arguments')
    sp_add = sps.add_parser('add', help='add a site')
    sp_add.add_argument('name', nargs=1, metavar='NAME', help='site name')
    sp_add.add_argument('link', nargs=1, metavar='URL', help='site url')
    sp_add.add_argument('depth', nargs='?', default='1', metavar='DEPTH', help='depth')
    sp_delete = sps.add_parser('delete', help='delete a site')
    sp_delete.add_argument('name', nargs=1, metavar='NAME', help='site name')
    sp_rename = sps.add_parser('rename', help='rename a site')
    sp_rename.add_argument('name', nargs=1, metavar='NAME', help='site name')
    sp_rename.add_argument('new_name', nargs=1, metavar='NEW_NAME', help='new name')
    sp_config = sps.add_parser('config', help='configure a site')
    sp_config.add_argument('name', nargs='*', metavar='NAME', help='site name')
    sp_config.add_argument('--link', '-l', nargs=1, metavar='URL', help='link')
    sp_config.add_argument('--depth', '-d', nargs=1, metavar='N', help='depth')
    sp_config.add_argument('--ignores', '-i', action='append', metavar='URL', help='add to ignore list')
    sp_config.add_argument('--remove-ignores', '-r', action='append', metavar='URL', help='remove from ignore list')
    sp_update = sps.add_parser('update', help='update a site or all sites')
    sp_update.add_argument('name', nargs=1, metavar='NAME', help='site name (\'all\' for all sites)')
    sp_links = sps.add_parser('links', help='print all links of a site or all sites')
    sp_links.add_argument('name', nargs=1, metavar='NAME', help='site name (\'all\' for all sites)')
    sp_links.add_argument('--sequence', '-s', default=None, metavar='N', help='time sequence number (0-9, latest=0, all by default)')
    sp_print = sps.add_parser('print', help='print latest links of a site or all sites')
    sp_print.add_argument('name', nargs=1, metavar='NAME', help='site name (\'all\' for all sites)')
    sp_print.add_argument('--sequence', '-s', default='0', metavar='N', help='time sequence number (0-9, latest=0 by deafult)')
    sp_print.add_argument('--slack', action='store_true', help='send to slack')
    sp_print.add_argument('--channel', nargs=1, metavar='CHANNEL', help='slack channel')
    sp_sequences = sps.add_parser('sequences', help='list time sequences')
    sp_sequences.add_argument('name', nargs=1, metavar='NAME', help='site name')
    sp_list = sps.add_parser('list', help='list sites')
    sp_export = sps.add_parser('export', help='export database')
    sp_import = sps.add_parser('import', help='import database')

    if len(sys.argv) == 1:
        print(parser.format_usage(), file=sys.stderr)
        exit(0)

    args = parser.parse_args()
    debug_mode = args.debug

    log_format = '%(levelname)s: %(message)s'
    log_level = logging.INFO
    if debug_mode is True:
        log_format = '%(asctime)s %(name)-12s %(levelname)-8s %(message)s'
        log_level = logging.DEBUG
    console = logging.StreamHandler()
    console.setFormatter(logging.Formatter(log_format))
    console.setLevel(log_level)
    logger.addHandler(console)

    if args.subparser_name == 'add':
        name = args.name[0].strip()
        link = args.link[0].strip()
        depth = int(args.depth)
        Site(name).add(link, depth)
    elif args.subparser_name == 'delete':
        name = args.name[0].strip()
        Site(name).delete()
    elif args.subparser_name == 'rename':
        name = args.name[0].strip()
        Site(name).rename(args.new_name[0])
    elif args.subparser_name == 'config':
        if len(args.name) == 0:
            SiteList().config(args.link, args.depth, args.ignores, args.remove_ignores)
        else:
            name = args.name[0].strip()
            Site(name).config(args.link, args.depth, args.ignores, args.remove_ignores)
    elif args.subparser_name == 'update':
        name = args.name[0].strip()
        if name.lower() == 'all':
            SiteList().update_all()
        else:
            Site(name).update()
    elif args.subparser_name == 'links':
        name = args.name[0].strip()
        if name.lower() == 'all':
            SiteList().links_all(args.sequence)
        else:
            Site(name).links(args.sequence)
    elif args.subparser_name == 'print':
        if args.slack is True:
            from ifslack import SlackInterface
            interface = SlackInterface()
            if args.channel is not None:
                channel = '#' + args.channel[0]
            else:
                channel = '#general'
        else:
            interface = PrintInterface()
            channel = None
        name = args.name[0].strip()
        if name.lower() == 'all':
            SiteList().print_all(args.sequence, interface, channel)
        else:
            Site(name).print(args.sequence, interface, channel)
    elif args.subparser_name == 'sequences':
        name = args.name[0].strip()
        if name.lower() == 'all':
            SiteList().sequences_all()
        else:
            Site(name).sequences()
    elif args.subparser_name == 'list':
        SiteList().list()
    elif args.subparser_name == 'export':
        SiteList().export_data()
    elif args.subparser_name == 'import':
        SiteList().import_data()
